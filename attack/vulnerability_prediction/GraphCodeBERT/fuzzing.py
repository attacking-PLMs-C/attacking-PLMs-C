import os
import sys
import torch
from collections import Counter
import tqdm
import argparse
import pickle as pkl
import json
from torchsummary import summary 
import re
import multiprocessing
import random
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer
from utils import *
import pandas as pd
import warnings
import torch.nn.functional as F
import numpy as np
from sklearn import metrics
from importlib import import_module
import operator
from Encoder import Model
from torch.nn import DataParallel
sys.path.append('.')
import glob
import logging
import pickle
import shutil
sys.path.append('.')
sys.path.append('./python_parser')
from run_parser import extract_dataflow
from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset
from torch.utils.data.distributed import DistributedSampler
try:
    from torch.utils.tensorboard import SummaryWriter
except:
    from tensorboardX import SummaryWriter
from Encoder import Model
cpu_cont = multiprocessing.cpu_count()
from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,
                          BertConfig, BertForMaskedLM, BertTokenizer,
                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,
                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,
                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,
                          DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)
MODEL_CLASSES = {
    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),
    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),
    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),
    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),
    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)
}


surface_mutatants = {'0': '1'}
syntax_mutatants = {'1': '1_12', '2': '12'}
# syntax_mutatants = {'1': '2', '2': '7', '4': '1_2', '5': '1_7', '7': '1_2_7', '11': '2_7'}
semantic_mutatants = {'3': '2', '4': '3', '5': '7', '6': '1_2', '7': '1_3', '8': '1_7', '9': '1_2_3', '10': '1_2_7', '11': '1_3_7', '12': '1_2_3_7', '13': '12_2', '14': '12_3', '15': '12_7',
                      '16': '12_2_3', '17': '12_2_7', '18': '12_3_7', '19': '12_2_3_7', '20': '1_12_2', '21': '1_12_3', '22': '1_12_7', '23': '1_12_2_3', '24': '1_12_2_7', '25': '1_12_3_7',
                      '26': '1_12_2_3_7', '27': '2_3', '28': '2_7', '29': '3_7', '30': '2_3_7'}

def get_candidates(args, initial_seeds, tokenizer, model, config, initial_candidates, indices, mutatants, file_to_label):
    model.eval()
    with torch.no_grad():
        for fc, seed in tqdm.tqdm(initial_seeds.items()):
            sub_features = []
            # if len(fc.split('-')) == 3:
            if len(fc.split('-')) == 1:
                s_name = fc
            else:
                s_name = fc.split('-')[0] + '.c'
            s_path = './initial_code/' + s_name
            s_code = open(s_path, 'r').read().strip()
            s_data = TextDataset(tokenizer, args, s_code, s_path, int(file_to_label[s_name]))
            s_dataloader = DataLoader(s_data, batch_size=1)
            for batch in s_dataloader:
                input_ids = batch[0].to(args.device)
                atten_mask = batch[1].to(args.device)
                position_idx = batch[2].to(args.device)
                label = batch[3].to(args.device)
                _, _, s_hiddenout, _ = model(input_ids, atten_mask, position_idx, label)
            angle_out = torch.index_select(s_hiddenout, dim=1, index=indices)[:,:,0,:]
            # E_out = torch.index_select(s_attentionout, dim=1, index=indices-1)
                    
            for sub_seed in seed:
                f_path = './mutated_candidates/mutated_code' + mutatants[sub_seed] + '/' + fc
                f_code = open(f_path, 'r').read().strip()
                
                f_data = TextDataset(tokenizer, args, f_code, f_path, int(file_to_label[s_name]))
                f_dataloader = DataLoader(f_data, batch_size=1)
                for f_batch in f_dataloader:
                    input_ids = f_batch[0].to(args.device)
                    atten_mask = f_batch[1].to(args.device)
                    position_idx = f_batch[2].to(args.device)
                    label = f_batch[3].to(args.device)
                    _, logit, f_hiddenout, _ = model(input_ids, atten_mask, position_idx, label)
                    f_predict = int(logit.cpu().numpy()>0.5)
                angle_out1 = torch.index_select(f_hiddenout, dim=1, index=indices)[:,:,0,:]
                # E_out1 = torch.index_select(f_attentionout, dim=1, index=indices-1)
                metric = F.cosine_similarity(angle_out, angle_out1, dim=-1)
                metric = torch.acos(metric) * 180 / 3.1415926
                metric = torch.mean(metric)
                if metric <= 0.0:
                    continue
                feat1 = [f_path, metric.item(), f_predict]
                sub_features.append(feat1)
                    
            # if fc.split('-')[0]+'.c' in initial_seeds:
            #     for sub_seed in initial_seeds[fc.split('-')[0]+'.c']:
            #         f_path1 = './mutated_candidates/mutated_code' + mutatants[sub_seed] + '/' + fc.split('-')[0] + '.c'
            #         f_code1 = open(f_path1, 'r').read().strip()
            #         f_predict1, f_hiddenout1, f_attentionout1 = get_prediction(f_code1, tokenizer, model, config)
            #         angle_out2 = torch.index_select(f_hiddenout1, dim=1, index=indices)[:,:,0,:]
            #         E_out2 = torch.index_select(f_attentionout1, dim=1, index=indices-1)
            #         metric1 = F.cosine_similarity(angle_out, angle_out2, dim=-1)
            #         metric1 = torch.acos(metric1) * 180 / 3.1415926
            #         metric1 = torch.mean(metric1)
            #         if metric1 <= 0.0:
            #             continue
            #         feat2 = [f_path1, metric1.item(), f_predict1]
            #         sub_features.append(feat2)
                    
            initial_candidates[s_name] = sorted(sub_features, key=operator.itemgetter(1), reverse=True)[:20]
    return initial_candidates


# def get_candidates(args, initial_seeds, tokenizer, model, config, initial_candidates, indices, mutatants, file_to_label):
#     model.eval()
#     with torch.no_grad():
#         for fc, seed in tqdm.tqdm(initial_seeds.items()):
#             sub_features = []
#             if len(fc.split('-')) == 3:
#                 s_path = './initial_code/' + fc.split('-')[0] + '.c'
#                 s_name = fc.split('-')[0] + '.c'
#                 s_code = open(s_path, 'r').read().strip()
#                 s_data = TextDataset(tokenizer, args, s_code, s_path, int(file_to_label[s_name]))
#                 s_dataloader = DataLoader(s_data, batch_size=1)
#                 for batch in s_dataloader:
#                     input_ids = batch[0].to(args.device)
#                     atten_mask = batch[1].to(args.device)
#                     position_idx = batch[2].to(args.device)
#                     label = batch[3].to(args.device)
#                     idx = batch[4]
#                     _, _, s_hiddenout, s_attentionout = model(input_ids, atten_mask, position_idx, label)
#                 angle_out = torch.index_select(s_hiddenout, dim=1, index=indices)[:,:,0,:]
#                 E_out = torch.index_select(s_attentionout, dim=1, index=indices-1)
                        
#                 for sub_seed in seed:
#                     f_path = './mutated_candidates/mutated_code' + mutatants[sub_seed] + '/' + fc
#                     f_code = open(f_path, 'r').read().strip()
#                     f_data = TextDataset(tokenizer, args, f_code, f_path, int(file_to_label[s_name]))
#                     f_dataloader = DataLoader(f_data, batch_size=1)
#                     for f_batch in f_dataloader:
#                         input_ids = f_batch[0].to(args.device)
#                         atten_mask = f_batch[1].to(args.device)
#                         position_idx = f_batch[2].to(args.device)
#                         label = f_batch[3].to(args.device)
#                         idx = f_batch[4]
#                         _, logit, f_hiddenout, f_attentionout = model(input_ids, atten_mask, position_idx, label)
#                         f_predict = int(logit.cpu().numpy()>0.5)
#                     angle_out1 = torch.index_select(f_hiddenout, dim=1, index=indices)[:,:,0,:]
#                     E_out1 = torch.index_select(f_attentionout, dim=1, index=indices-1)
#                     metric = torch.norm(E_out-E_out1, p=2)
#                     if metric <= 0.0:
#                         continue
#                     feat1 = [f_path, metric.item(), f_predict]
#                     sub_features.append(feat1)
                        
#                 if fc.split('-')[0]+'.c' in initial_seeds:
#                     for sub_seed in initial_seeds[fc.split('-')[0]+'.c']:
#                         f_path1 = './mutated_candidates/mutated_code' + mutatants[sub_seed] + '/' + fc.split('-')[0] + '.c'
#                         f_code1 = open(f_path1, 'r').read().strip()
#                         f_data1 = TextDataset(tokenizer, args, f_code1, f_path1, int(file_to_label[s_name]))
#                         f_dataloader1 = DataLoader(f_data1, batch_size=1)
#                         for f_batch1 in f_dataloader1:
#                             input_ids = f_batch1[0].to(args.device)
#                             atten_mask = f_batch1[1].to(args.device)
#                             position_idx = f_batch1[2].to(args.device)
#                             label = f_batch1[3].to(args.device)
#                             idx = f_batch1[4]
#                             _, logit, f_hiddenout1, f_attentionout1 = model(input_ids, atten_mask, position_idx, label)
#                             f_predict1 = int(logit.cpu().numpy()>0.5)
#                         angle_out2 = torch.index_select(f_hiddenout1, dim=1, index=indices)[:,:,0,:]
#                         E_out2 = torch.index_select(f_attentionout1, dim=1, index=indices-1)
#                         metric1 = F.cosine_similarity(angle_out, angle_out2, dim=-1)
#                         metric1 = torch.acos(metric1) * 180 / 3.1415926
#                         metric1 = torch.mean(metric1)
#                         if metric1 <= 0.0:
#                             continue
#                         feat2 = [f_path1, metric1.item(), f_predict1]
#                         sub_features.append(feat2)
                    
#                 initial_candidates[fc.split('-')[0]+'.c'] = sorted(sub_features, key=operator.itemgetter(1), reverse=True)
#     return initial_candidates

def get_initial_seeds(file, t_file, k, v, initial_seeds):
    if file in v and file not in initial_seeds:
        initial_seeds[file] = [k]
    elif t_file in v and t_file not in initial_seeds:
        initial_seeds[t_file] = [k]
    elif file in v and file in initial_seeds:
        initial_seeds[file].append(k)
    elif t_file in v and t_file in initial_seeds:
        initial_seeds[t_file].append(k)
    return initial_seeds

def fit_function(args, file_to_label, m_file, source_path, tmp_path, tokenizer, model, config, liguistic_mutatants, liguistic_indices, idx, tag=False):
    if tag == True:
        fname = m_file
    else:
        fname = m_file.split('-')[0] + '.c'
    if len(m_file.split('-')) > 1:
        s_name = m_file.split('-')[0] + '.c'
    else:
        s_name = m_file
    s_code = open(source_path+fname, 'r').read().strip()
    s_data = TextDataset(tokenizer, args, s_code, source_path+fname, int(file_to_label[s_name]))
    s_dataloader = DataLoader(s_data, batch_size=1)
    for s_batch in s_dataloader:
        input_ids = s_batch[0].to(args.device)
        atten_mask = s_batch[1].to(args.device)
        position_idx = s_batch[2].to(args.device)
        label = s_batch[3].to(args.device)
        # idx = f_batch[4]
        _, _, s_hiddenout, s_attentionout = model(input_ids, atten_mask, position_idx, label)
    angle_out = torch.index_select(s_hiddenout, dim=1, index=liguistic_indices)[:,:,0,:]
    E_out = torch.index_select(s_attentionout, dim=1, index=liguistic_indices-1)

    tmp_code = open(tmp_path+'/'+m_file, 'r').read().strip()
    tmp_data = TextDataset(tokenizer, args, tmp_code, tmp_path+'/'+m_file, int(file_to_label[s_name]))
    tmp_dataloader = DataLoader(tmp_data, batch_size=1)
    for tmp_batch in tmp_dataloader:
        input_ids = tmp_batch[0].to(args.device)
        atten_mask = tmp_batch[1].to(args.device)
        position_idx = tmp_batch[2].to(args.device)
        label = tmp_batch[3].to(args.device)
        # idx = tmp_batch[4]
        _, _, tmp_hiddenout, tmp_attentionout = model(input_ids, atten_mask, position_idx, label)
    tmp_angle_out = torch.index_select(tmp_hiddenout, dim=1, index=liguistic_indices)[:,:,0,:]
    tmp_E_out = torch.index_select(tmp_attentionout, dim=1, index=liguistic_indices-1)

    t_code = open('./mutated_candidates/mutated_code'+liguistic_mutatants[str(idx)]+'/'+m_file, 'r').read().strip()
    t_data = TextDataset(tokenizer, args, t_code, './mutated_candidates/mutated_code'+liguistic_mutatants[str(idx)]+'/'+m_file, int(file_to_label[s_name]))
    t_dataloader = DataLoader(t_data, batch_size=1)
    for t_batch in t_dataloader:
        input_ids = t_batch[0].to(args.device)
        atten_mask = t_batch[1].to(args.device)
        position_idx = t_batch[2].to(args.device)
        label = t_batch[3].to(args.device)
        # idx = f_batch[4]
        _, _, t_hiddenout, t_attentionout = model(input_ids, atten_mask, position_idx, label)
    t_angle_out = torch.index_select(t_hiddenout, dim=1, index=liguistic_indices)[:,:,0,:]
    t_E_out = torch.index_select(t_attentionout, dim=1, index=liguistic_indices-1)

    cs_value = F.cosine_similarity(angle_out, tmp_angle_out, dim=-1)
    adho = torch.acos(cs_value) * 180 / 3.1415926
    adho = torch.mean(adho)
    edao = torch.norm(E_out-tmp_E_out, p=2)

    cs_value1 = F.cosine_similarity(tmp_angle_out, t_angle_out, dim=-1)
    adho1 = torch.acos(cs_value1) * 180 / 3.1415926
    adho1 = torch.mean(adho1)
    edao1 = torch.norm(tmp_E_out-t_E_out, p=2)

    if adho1 >= adho and edao1 >= edao:
        return True
    else:
        return False

def fuzzer(config, args, model, tokenizer, file_to_label):
    model.eval()
    surface_files = []
    code_rename = []
    tmp = []
    for_to_while = []
    switch = []
    code_redefine = []
    # true_mutated_files = {}
    surface_mutated_files = {}
    syntax_mutated_files = {}
    semantic_mutated_files = {}
    sub_query = 0.0
    
    surface_indices = torch.tensor([5]).to(args.device)
    syntax_indices = torch.tensor([5,6,7]).to(args.device)
    semantic_indices = torch.tensor([8,9,10]).to(args.device)
    get_file_list('./mutated_candidates/true_mutated1.txt', tmp)

    variable_candidates = {}
    surface_success = []
    syntax_sucess = []

    code_rename = []
    if os.path.exists('./surface_seeds.json'):
        surface_seeds = json.load(open('./surface_seeds.json'))
        syntax_seeds = json.load(open('./syntax_seeds.json'))
        semantic_seeds = json.load(open('./semantic_seeds.json'))
    else:
        with torch.no_grad():
            for t_file in tqdm.tqdm(tmp):
                t_name = t_file.split('-')[0] + '-' + t_file.split('-')[1] + '.c'
                if t_name not in variable_candidates:
                    variable_candidates[t_name] = [t_file]
                else:
                    variable_candidates[t_name].append(t_file)
                if t_name not in code_rename:
                    code_rename.append(t_name)
            # print(variable_candidates)
            # print(code_rename, len(code_rename))

            surface_files = json.load(open('./surface_files.json'))
            # if os.path.exists('./file_to_candidates.json'):
            #     file_to_candidates = json.load(open('./file_to_candidates.json'))
            # else:
            #     file_to_candidates = {}
            #     for file in tqdm.tqdm(code_rename):
            #         s_name = file.split('-')[0] + '.c'
            #         s_path = './initial_code/' + s_name
            #         s_code = open(s_path, 'r').read().strip()
            #         s_data = TextDataset(tokenizer, args, s_code, s_path, int(file_to_label[s_name]))
            #         # s_sampler = SequentialSampler(s_data)
            #         s_dataloader = DataLoader(s_data, batch_size=1)
            #         for batch in s_dataloader:
            #             input_ids = batch[0].to(args.device)
            #             atten_mask = batch[1].to(args.device)
            #             position_idx = batch[2].to(args.device)
            #             label = batch[3].to(args.device)
            #             idx = batch[4]
            #             _, logit, s_hiddenout, s_attentionout = model(input_ids, atten_mask, position_idx, label)
            #             lab = int(logit.cpu().numpy()>0.5)
            #         angle_out = torch.index_select(s_hiddenout, dim=1, index=surface_indices)[:,:,0,:]
            #         E_out = torch.index_select(s_attentionout, dim=1, index=surface_indices-1)

            #         '''从三个候选变量中选取最合适的一个'''
            #         max_distance1 = 0.0
            #         max_distance2 = 0.0
            #         best_cand = ''
            #         tag1 = True
            #         for sub_cand in variable_candidates[file]:
            #             sub_path = './mutated_candidates/mutated_code1/' + sub_cand
            #             sub_code = open(sub_path, 'r').read().strip()
            #             sub_data = TextDataset(tokenizer, args, sub_code, sub_path, int(file_to_label[s_name]))
            #             sub_dataloader = DataLoader(sub_data, batch_size=1)
            #             for sub_batch in sub_dataloader:
            #                 input_ids = sub_batch[0].to(args.device)
            #                 atten_mask = sub_batch[1].to(args.device)
            #                 position_idx = sub_batch[2].to(args.device)
            #                 label = sub_batch[3].to(args.device)
            #                 idx = sub_batch[4]
            #                 _, logit, sub_hiddenout, sub_attentionout = model(input_ids, atten_mask, position_idx, label)
            #                 pred = int(logit.cpu().numpy()>0.5)
            #             sub_angle_out = torch.index_select(sub_hiddenout, dim=1, index=surface_indices)[:,:,0,:]
            #             sub_E_out = torch.index_select(sub_attentionout, dim=1, index=surface_indices-1)
            #             if lab != pred:
            #                 best_cand = sub_cand
            #                 break
                        
            #             metric1 = F.cosine_similarity(angle_out, sub_angle_out, dim=-1)
            #             metric1 = torch.acos(metric1) * 180 / 3.1415926
            #             metric1 = torch.mean(metric1)
            #             metric2 = torch.norm(E_out-sub_E_out, p=2)
            #             if metric1 <= 0.0 or metric2 <= 0.0:
            #                 tag1 = False
            #                 break
            #             if max_distance1 < metric1 or max_distance2 < metric2:
            #                 max_distance1 = metric1
            #                 max_distance2 = metric2
            #         if tag1 == False:
            #             continue
            #         if s_name not in file_to_candidates:
            #             file_to_candidates[s_name] = [best_cand]
            #         elif s_name in file_to_candidates:
            #             file_to_candidates[s_name].append(best_cand)
            #         with open('./file_to_candidates.json', 'w') as fp:
            #             json.dump(file_to_candidates, fp)
            #     # print(file_to_candidates)

            # tmcode = []
            # initial_mutated = open('true_mutated.txt', 'w')
            # for fname, cand_list in tqdm.tqdm(file_to_candidates.items()):
            #     for i in range(len(cand_list)):
            #         if cand_list[i] == '':
            #             continue
            #         sub_query += 1.0
            #         if i == len(cand_list) - 1:
            #             surface_files.append(cand_list[i])
            #             initial_mutated.write(cand_list[i] + '\n')
            #             tmcode.append(cand_list[i])
            #             break
            #         f_path = './mutated_candidates/mutated_code1/' + cand_list[i]
            #         f_code = open(f_path, 'r').read().strip()
            #         f_data = TextDataset(tokenizer, args, f_code, f_path, int(file_to_label[fname]))
            #         f_dataloader = DataLoader(f_data, batch_size=1)
            #         for f_batch in f_dataloader:
            #             input_ids = f_batch[0].to(args.device)
            #             atten_mask = f_batch[1].to(args.device)
            #             position_idx = f_batch[2].to(args.device)
            #             label = f_batch[3].to(args.device)
            #             idx = f_batch[4]
            #             _, logit, _, _ = model(input_ids, atten_mask, position_idx, label)
            #             predict = int(logit.cpu().numpy()>0.5)
            #         if predict != int(file_to_label[fname]):
            #             surface_files.append(cand_list[i])
            #             tmcode.append(cand_list[i])
            #             initial_mutated.write(cand_list[i] + '\n')
            #             break
            
            '''通过ADHO和EDAO生成新种子'''
            mutatant_seed = {'1': '0', '1_12': '1', '12': '2'}
            for key, value in semantic_mutatants.items():
                mutatant_seed[value] = key
            
            all_mutated_files = {}
            for i in tqdm.tqdm(range(31)):
                if str(i) in surface_mutatants:
                    source_path = './mutated_candidates/true_mutated' + surface_mutatants[str(i)] + '.txt'
                    surface_mutated_files[str(i)] = get_file_list(source_path, [])
                    all_mutated_files[str(i)] = get_file_list(source_path, [])
                elif str(i) in syntax_mutatants:
                    m_seeds = syntax_mutatants[str(i)].split('_')
                    if len(m_seeds) == 1:
                        source_path = './mutated_candidates/true_mutated' + syntax_mutatants[str(i)] + '.txt'
                        syntax_mutated_files[str(i)] = get_file_list(source_path, [])
                        all_mutated_files[str(i)] = get_file_list(source_path, [])
                    else:
                        syntax_mutated_files[str(i)] = []
                        all_mutated_files[str(i)] = []
                        target_path = './mutated_candidates/true_mutated' + syntax_mutatants[str(i)] + '.txt'
                        target_mutatants = get_file_list(target_path, [])
                        source_path = './initial_code/'
                        tmp_path = './mutated_candidates/mutated_code' + syntax_mutatants[str(i)][:-(len(m_seeds[-1])+1)]
                        for m_file in target_mutatants:
                            if m_file in all_mutated_files[mutatant_seed[syntax_mutatants[str(i)][:-(len(m_seeds[-1])+1)]]]:
                                if fit_function(args, file_to_label, m_file, source_path, tmp_path, tokenizer, model, config, syntax_mutatants, syntax_indices, i):
                                    syntax_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)

                elif str(i) in semantic_mutatants:
                    m_seeds = semantic_mutatants[str(i)].split('_')
                    if len(m_seeds) == 1:
                        source_path = './mutated_candidates/true_mutated' + semantic_mutatants[str(i)] + '.txt'
                        semantic_mutated_files[str(i)] = get_file_list(source_path, [])
                        all_mutated_files[str(i)] = get_file_list(source_path, [])
                    elif len(m_seeds) == 2:
                        semantic_mutated_files[str(i)] = []
                        all_mutated_files[str(i)] = []
                        target_path = './mutated_candidates/true_mutated' + semantic_mutatants[str(i)] + '.txt'
                        target_mutatants = get_file_list(target_path, [])
                        source_path = './initial_code/'
                        tmp_path = './mutated_candidates/mutated_code' + semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]
                        for m_file in target_mutatants:
                            if m_file in all_mutated_files[mutatant_seed[semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]]]:
                                if i <= 9 and fit_function(args, file_to_label, m_file, source_path, tmp_path, tokenizer, model, config, semantic_mutatants, semantic_indices, i):
                                    semantic_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)
                                elif i > 9 and fit_function(args, file_to_label, m_file, source_path, tmp_path, tokenizer, model, config, semantic_mutatants, semantic_indices, i, True):
                                    semantic_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)
                    else:
                        semantic_mutated_files[str(i)] = []
                        all_mutated_files[str(i)] = []
                        target_path = './mutated_candidates/true_mutated' + semantic_mutatants[str(i)] + '.txt'
                        target_mutatants = get_file_list(target_path, [])
                        source_path = './mutated_candidates/mutated_code' + semantic_mutatants[str(i)][:-(len(m_seeds[-2])+1+len(m_seeds[-1])+1)] + '/'
                        # source_path = './initial_code/'
                        tmp_path = './mutated_candidates/mutated_code' + semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]
                        for m_file in target_mutatants:
                            if m_file in all_mutated_files[mutatant_seed[semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]]]:
                                if fit_function(args, file_to_label, m_file, source_path, tmp_path, tokenizer, model, config, semantic_mutatants, semantic_indices, i, True):
                                    semantic_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)

            surface_seeds = {}
            syntax_seeds = {}
            semantic_seeds = {}
            for file in tqdm.tqdm(surface_files):
                t_file = file.split('-')[0] + '.c'
                for k, v in surface_mutated_files.items():
                    get_initial_seeds(file, t_file, k, v, surface_seeds)
                for k, v in syntax_mutated_files.items():
                    get_initial_seeds(file, t_file, k, v, syntax_seeds)
                for k, v in semantic_mutated_files.items():
                    get_initial_seeds(file, t_file, k, v, semantic_seeds)
            with open('./surface_seeds.json', 'w') as fp:
                json.dump(surface_seeds, fp)
            with open('./syntax_seeds.json', 'w') as fp:
                json.dump(syntax_seeds, fp)
            with open('./semantic_seeds.json', 'w') as fp:
                json.dump(semantic_seeds, fp)

        # print(initial_seeds)
        # for k, v in initial_seeds.items():
        #     if k == 'a553c6a347d3d28d7ee44c3df3d5c4ee780dba23_10398.c' or k.split('-')[0]+'.c' == 'a553c6a347d3d28d7ee44c3df3d5c4ee780dba23_10398.c':
        #         print(k)
    surface_candidates = {}
    syntax_candidates = {}
    semantic_candidates = {}
    # atten_candidates = {}
    print('[+] generating adversarial examples..')
    print(len(code_rename))
    get_candidates(args, surface_seeds, tokenizer, model, config, surface_candidates, surface_indices, surface_mutatants, file_to_label)   
    get_candidates(args, syntax_seeds, tokenizer, model, config, syntax_candidates, syntax_indices, syntax_mutatants, file_to_label)
    get_candidates(args, semantic_seeds, tokenizer, model, config, semantic_candidates, semantic_indices, semantic_mutatants, file_to_label)

    # with open('surface_candidates.json', 'w') as fp:
    #     json.dump(surface_candidates, fp)
    print(len(surface_candidates), len(syntax_candidates), len(semantic_candidates)) 
    return surface_candidates, syntax_candidates, semantic_candidates, sub_query
    

def testing(surface_candidates, syntax_candidates, semantic_candidates, file_to_label, sub_query):
    query = 0.0
    testcases_num = 0.0
    success = 0.0
    
    '''surface层面的扰动'''
    syntax_initial_files = []
    for fname, features in tqdm.tqdm(surface_candidates.items()):
         for i, sub_features in enumerate(features):
            predict = sub_features[-1]
            query += 1.0
            # print(sub_features)
            if sub_features[-1] != int(file_to_label[fname]):
                testcases_num += 1.0
                success += 1.0
                break
            elif fname not in syntax_initial_files:
                syntax_initial_files.append(fname)

    '''syntax层面的扰动'''
    semantic_initial_files = []
    for fname in tqdm.tqdm(syntax_initial_files):
        if fname in syntax_candidates:
            for i, sub_features in enumerate(syntax_candidates[fname]):
                predict = sub_features[-1]
                query += 1.0
                if sub_features[-1] != int(file_to_label[fname]):
                    testcases_num += 1.0
                    success += 1.0
                    break
                elif fname not in semantic_initial_files:
                    semantic_initial_files.append(fname)
        else:
            semantic_initial_files.append(fname)
    
    '''semantic层面的扰动'''
    for fname in tqdm.tqdm(semantic_initial_files):
        if fname in semantic_candidates:
            for i, sub_features in enumerate(semantic_candidates[fname]):
                predict = sub_features[-1]
                query += 1.0
                if sub_features[-1] != int(file_to_label[fname]):
                    testcases_num += 1.0
                    success += 1.0
                    break
                elif i == 0:
                    testcases_num += 1.0
                    
    print('[+] Results: query numbr:{}, success examples:{}, all testcases:{}, attacking successful rate:{}'.format(query+sub_query, success, testcases_num, success / testcases_num))
    # testing_result.close()
            


def main():
    parser = argparse.ArgumentParser()

    ## Required parameters
    parser.add_argument("--output_dir", default="./saved_models", type=str,
                        help="The output directory where the model predictions and checkpoints will be written.")                  
    parser.add_argument("--model_type", default="roberta", type=str,
                        help="The model architecture to be fine-tuned.")
    parser.add_argument("--model_name_or_path", default='microsoft/graphcodebert-base', type=str,
                        help="The model checkpoint for weights initialization.")
    parser.add_argument("--config_name", default="microsoft/graphcodebert-base", type=str,
                        help="Optional pretrained config name or path if not the same as model_name_or_path")
    parser.add_argument("--tokenizer_name", default="microsoft/graphcodebert-base", type=str,
                        help="Optional pretrained tokenizer name or path if not the same as model_name_or_path")
    parser.add_argument("--cache_dir", default="", type=str,
                        help="Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)")
    parser.add_argument("--code_length", default=512, type=int,
                        help="Optional Code input sequence length after tokenization.") 
    parser.add_argument("--data_flow_length", default=128, type=int,
                        help="Optional Data Flow input sequence length after tokenization.")    
    parser.add_argument("--train_batch_size", default=32, type=int,
                        help="Batch size per GPU/CPU for training.")
    parser.add_argument('--seed', type=int, default=123456,
                        help="random seed for initialization")
    parser.add_argument("--eval_batch_size", default=32, type=int,
                        help="Batch size per GPU/CPU for evaluation.")
    parser.add_argument("--learning_rate", default=2e-5, type=float,
                        help="The initial learning rate for Adam.")
    parser.add_argument("--weight_decay", default=0.0, type=float,
                        help="Weight deay if we apply some.")
    parser.add_argument("--adam_epsilon", default=1e-8, type=float,
                        help="Epsilon for Adam optimizer.")
    parser.add_argument("--no_cuda", action='store_true',
                        help="Avoid using CUDA when available")
    parser.add_argument("--local_rank", type=int, default=-1,
                        help="For distributed training: local_rank")
    parser.add_argument('--d_type', type=str, default='a_distance')

    args = parser.parse_args()
    # Setup CUDA, GPU & distributed training
    if args.local_rank == -1 or args.no_cuda:
        device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
        args.n_gpu = 1
    args.device = device
    args.per_gpu_train_batch_size=args.train_batch_size//args.n_gpu
    args.per_gpu_eval_batch_size=args.eval_batch_size//args.n_gpu
    set_seed(args.seed)

    config_class, model_class, tokenizer_class = MODEL_CLASSES['roberta']
    config = config_class.from_pretrained('microsoft/graphcodebert-base', cache_dir=args.cache_dir if args.cache_dir else None)
    config.num_labels=1
    tokenizer = tokenizer_class.from_pretrained('microsoft/graphcodebert-base', do_lower_case=False, cache_dir=None)

    model = model_class.from_pretrained('microsoft/graphcodebert-base',
                                            from_tf=bool('.ckpt' in args.model_name_or_path),
                                            config=config,
                                            cache_dir=None)    

    model=Model(model,config,tokenizer,args)
    checkpoint_prefix = 'checkpoint-best-acc/model.bin'
    output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  
    model.load_state_dict(torch.load(output_dir))                  
    model.to(args.device)
    # for name, params in model.named_parameters():
    #     print(name, params)

    file_to_label = {}
    with open('code_labels.txt', 'r') as fp:
        all_lines = fp.read().strip().split('\n')
        for line in tqdm.tqdm(all_lines):
            fname = line.split('  ')[0]
            label  = line.split('  ')[-1]
            file_to_label[fname] = label
    
    # fuzzer(config, args, model, tokenizer, file_to_label)
    surface_candidates, syntax_candidates, semantic_candidates, sub_query = fuzzer(config, args, model, tokenizer, file_to_label)
    testing(surface_candidates, syntax_candidates, semantic_candidates, file_to_label, sub_query)

    # # atten_candidates, sub_query = fuzzer('e_distance', config, args, model, file_to_label)
    # # testing(2, atten_candidates, file_to_label, sub_query)

if __name__ == "__main__":
    main()