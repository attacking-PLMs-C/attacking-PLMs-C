import os
import sys
import torch
from collections import Counter
import tqdm
import argparse
import pickle as pkl
import json
import re
import random
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer

def get_tokenization(target_code, tokenizer):
    target_code = re.sub(r'(\s)+', ' ', target_code).strip()
    target_tokens = ''.join(target_code.split(' '))
    target_tokens = tokenizer.tokenize(target_tokens)[:510]
    target_tokens = [tokenizer.cls_token] + target_tokens + [tokenizer.sep_token]
    target_ids = tokenizer.convert_tokens_to_ids(target_tokens)
    padding_length = 512 - len(target_ids)
    target_ids += [tokenizer.pad_token_id] * padding_length

    return target_ids

def get_feature_vector(fcode, tokenizer):
    fcode = re.sub(r'(\s)+', ' ', fcode).strip()
    ftokens = ''.join(fcode.split(' '))
    ftokens = tokenizer.tokenize(ftokens)[:510]
    ftokens = [tokenizer.cls_token] + ftokens + [tokenizer.sep_token]
    fids = tokenizer.convert_tokens_to_ids(ftokens)
    padding_length = 512 - len(fids)
    fids += [tokenizer.pad_token_id] * padding_length
    return fids

def get_prediction(f_code, tokenizer, model, config):
    f_ids = get_feature_vector(f_code, tokenizer)
    f_ids = torch.tensor(f_ids).to(config.device)
    out, hidden_out, atten_out = model(f_ids)
    predict = torch.max(out.data, 1)[1].item()
    return predict, hidden_out, atten_out

def get_file_list(root_path, file_list):
    with open(root_path, 'r') as fp:
        all_lines = fp.read().strip().split('\n')
        for line in all_lines:
            file_list.append(line)
    return file_list

def get_file_path(root_path, file_list):
    PATH = os.listdir(root_path)
    for path in PATH:
        # print(path)
        co_path = os.path.join(root_path, path)
        if os.path.isfile(co_path):
            file_list.append(co_path)
        elif os.path.isdir(co_path):
            get_file_path(co_path, file_list)
    return file_list
