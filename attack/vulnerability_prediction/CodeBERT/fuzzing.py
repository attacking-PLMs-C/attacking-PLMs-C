import os
import sys
import torch
from collections import Counter
import tqdm
import argparse
import pickle as pkl
import json
import re
import random
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer
from utils import *
import pandas as pd
import subprocess
import warnings
import torch.nn.functional as F
import numpy as np
from sklearn import metrics
from importlib import import_module
import operator
from torch.nn import DataParallel
sys.path.append('.')

surface_mutatants = {'0': '1'}
syntax_mutatants = {'1': '1_12', '2': '12'}
# syntax_mutatants = {'1': '2', '2': '7', '4': '1_2', '5': '1_7', '7': '1_2_7', '11': '2_7'}
semantic_mutatants = {'3': '2', '4': '3', '5': '7', '6': '1_2', '7': '1_3', '8': '1_7', '9': '1_2_3', '10': '1_2_7', '11': '1_3_7', '12': '1_2_3_7', '13': '12_2', '14': '12_3', '15': '12_7',
                      '16': '12_2_3', '17': '12_2_7', '18': '12_3_7', '19': '12_2_3_7', '20': '1_12_2', '21': '1_12_3', '22': '1_12_7', '23': '1_12_2_3', '24': '1_12_2_7', '25': '1_12_3_7',
                      '26': '1_12_2_3_7', '27': '2_3', '28': '2_7', '29': '3_7', '30': '2_3_7'}
# semantic_mutatants = {'3': '12', '6': '1_12', '8': '1_2_12', '9': '1_7_12', '10': '1_2_7_12', '12': '2_12', '13': '2_7_12', '14': '7_12'}


def get_candidates(initial_seeds, tokenizer, model, config, initial_candidates, indices, mutatants):
    model.eval()
    with torch.no_grad():
        for fc, seed in tqdm.tqdm(initial_seeds.items()):
            sub_features = []
            # if len(fc.split('-')) == 3:
            if len(fc.split('-')) == 1:
                s_name = fc
            else:
                s_name = fc.split('-')[0] + '.c'
            s_path = './initial_code/' + s_name
            s_code = open(s_path, 'r').read().strip()
            _, s_hiddenout, s_attentionout = get_prediction(s_code, tokenizer, model, config)
            angle_out = torch.index_select(s_hiddenout, dim=1, index=indices)[:,:,0,:]
            E_out = torch.index_select(s_attentionout, dim=1, index=indices-1)
                    
            for sub_seed in seed:
                f_path = './mutated_candidates/mutated_code' + mutatants[sub_seed] + '/' + fc
                f_code = open(f_path, 'r').read().strip()
                f_predict, f_hiddenout, f_attentionout = get_prediction(f_code, tokenizer, model, config)
                angle_out1 = torch.index_select(f_hiddenout, dim=1, index=indices)[:,:,0,:]
                E_out1 = torch.index_select(f_attentionout, dim=1, index=indices-1)
                metric = F.cosine_similarity(angle_out, angle_out1, dim=-1)
                metric = torch.acos(metric) * 180 / 3.1415926
                metric = torch.mean(metric)
                if metric <= 0.0:
                    continue
                feat1 = [f_path, metric.item(), f_predict]
                sub_features.append(feat1)
                    
            # if fc.split('-')[0]+'.c' in initial_seeds:
            #     for sub_seed in initial_seeds[fc.split('-')[0]+'.c']:
            #         f_path1 = './mutated_candidates/mutated_code' + mutatants[sub_seed] + '/' + fc.split('-')[0] + '.c'
            #         f_code1 = open(f_path1, 'r').read().strip()
            #         f_predict1, f_hiddenout1, f_attentionout1 = get_prediction(f_code1, tokenizer, model, config)
            #         angle_out2 = torch.index_select(f_hiddenout1, dim=1, index=indices)[:,:,0,:]
            #         E_out2 = torch.index_select(f_attentionout1, dim=1, index=indices-1)
            #         metric1 = F.cosine_similarity(angle_out, angle_out2, dim=-1)
            #         metric1 = torch.acos(metric1) * 180 / 3.1415926
            #         metric1 = torch.mean(metric1)
            #         if metric1 <= 0.0:
            #             continue
            #         feat2 = [f_path1, metric1.item(), f_predict1]
            #         sub_features.append(feat2)
                    
            initial_candidates[s_name] = sorted(sub_features, key=operator.itemgetter(1), reverse=True)[:20]
    return initial_candidates

def get_initial_seeds(file, t_file, k, v, initial_seeds):
    if file in v and file not in initial_seeds:
        initial_seeds[file] = [k]
    elif t_file in v and t_file not in initial_seeds:
        initial_seeds[t_file] = [k]
    elif file in v and file in initial_seeds:
        initial_seeds[file].append(k)
    elif t_file in v and t_file in initial_seeds:
        initial_seeds[t_file].append(k)
    return initial_seeds

def fit_function(m_file, source_path, tmp_path, tokenizer, model, config, liguistic_mutatants, liguistic_indices, idx, tag=False):
    if tag == True:
        fname = m_file
    else:
        fname = m_file.split('-')[0] + '.c'
    s_code = open(source_path+fname, 'r').read().strip()
    _, s_hiddenout, s_attentionout = get_prediction(s_code, tokenizer, model, config)
    angle_out = torch.index_select(s_hiddenout, dim=1, index=liguistic_indices)[:,:,0,:]
    E_out = torch.index_select(s_attentionout, dim=1, index=liguistic_indices-1)

    tmp_code = open(tmp_path+'/'+m_file, 'r').read().strip()
    _, tmp_hiddenout, tmp_attentionout = get_prediction(tmp_code, tokenizer, model, config)
    tmp_angle_out = torch.index_select(tmp_hiddenout, dim=1, index=liguistic_indices)[:,:,0,:]
    tmp_E_out = torch.index_select(tmp_attentionout, dim=1, index=liguistic_indices-1)

    t_code = open('./mutated_candidates/mutated_code'+liguistic_mutatants[str(idx)]+'/'+m_file, 'r').read().strip()
    _, t_hiddenout, t_attentionout = get_prediction(t_code, tokenizer, model, config)
    t_angle_out = torch.index_select(t_hiddenout, dim=1, index=liguistic_indices)[:,:,0,:]
    t_E_out = torch.index_select(t_attentionout, dim=1, index=liguistic_indices-1)

    cs_value = F.cosine_similarity(angle_out, tmp_angle_out, dim=-1)
    adho = torch.acos(cs_value) * 180 / 3.1415926
    adho = torch.mean(adho)
    edao = torch.norm(E_out-tmp_E_out, p=2)

    cs_value1 = F.cosine_similarity(tmp_angle_out, t_angle_out, dim=-1)
    adho1 = torch.acos(cs_value1) * 180 / 3.1415926
    adho1 = torch.mean(adho1)
    edao1 = torch.norm(tmp_E_out-t_E_out, p=2)

    if adho1 >= adho and edao1 >= edao:
        return True
    else:
        return False

def fuzzer(config, args, model, file_to_label):
    model.eval()
    surface_files = []
    code_rename = []
    tmp = []
    for_to_while = []
    switch = []
    code_redefine = []
    # true_mutated_files = {}
    surface_mutated_files = {}
    syntax_mutated_files = {}
    semantic_mutated_files = {}
    sub_query = 0.0
    tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
    
    surface_indices = torch.tensor([5]).to(config.device)
    syntax_indices = torch.tensor([5,6,7]).to(config.device)
    semantic_indices = torch.tensor([8,9,10]).to(config.device)
    get_file_list('./mutated_candidates/true_mutated1.txt', tmp)

    variable_candidates = {}
    surface_success = []
    syntax_sucess = []

    if os.path.exists('./surface_seeds.json'):
        surface_seeds = json.load(open('./surface_seeds.json'))
        syntax_seeds = json.load(open('./syntax_seeds.json'))
        semantic_seeds = json.load(open('./semantic_seeds.json'))
    else:
        code_rename = []
        with torch.no_grad():
            for t_file in tqdm.tqdm(tmp):
                t_name = t_file.split('-')[0] + '-' + t_file.split('-')[1] + '.c'
                if t_name not in variable_candidates:
                    variable_candidates[t_name] = [t_file]
                else:
                    variable_candidates[t_name].append(t_file)
                if t_name not in code_rename:
                    code_rename.append(t_name)
            # print(variable_candidates)
            # print(code_rename, len(code_rename))
            
            surface_files = json.load(open('./surface_files.json'))
            # if os.path.exists('file_to_candidates.json'):
            #     file_to_candidates = json.load(open('./file_to_candidates.json'))
            # else:
            #     file_to_candidates = {}
            #     for file in tqdm.tqdm(code_rename):
            #         s_name = file.split('-')[0] + '.c'
            #         s_path = './initial_code/' + s_name
            #         s_code = open(s_path, 'r').read().strip()
            #         lab, s_hiddenout, s_attentionout = get_prediction(s_code, tokenizer, model, config)
            #         angle_out = torch.index_select(s_hiddenout, dim=1, index=surface_indices)[:,:,0,:]
            #         E_out = torch.index_select(s_attentionout, dim=1, index=surface_indices-1)

            #         '''从三个候选变量中选取最合适的一个'''
            #         max_distance1 = 0.0
            #         max_distance2 = 0.0
            #         best_cand = ''
            #         tag1 = True
            #         for sub_cand in variable_candidates[file]:
            #             sub_path = './mutated_candidates/mutated_code1/' + sub_cand
            #             sub_code = open(sub_path, 'r').read().strip()
            #             pred, sub_hiddenout, sub_attentionout = get_prediction(sub_code, tokenizer, model, config)
            #             sub_angle_out = torch.index_select(sub_hiddenout, dim=1, index=surface_indices)[:,:,0,:]
            #             sub_E_out = torch.index_select(sub_attentionout, dim=1, index=surface_indices-1)
            #             if lab != pred:
            #                 best_cand = sub_cand
            #                 break
            #             metric1 = F.cosine_similarity(angle_out, sub_angle_out, dim=-1)
            #             metric1 = torch.acos(metric1) * 180 / 3.1415926
            #             metric1 = torch.mean(metric1)
            #             metric2 = torch.norm(E_out-sub_E_out, p=2)
            #             if metric1 <= 0.0 or metric2 <= 0.0:
            #                 tag1 = False
            #                 break
            #             if max_distance1 < metric1 or max_distance2 < metric2:
            #                 max_distance1 = metric1
            #                 max_distance2 = metric2
            #                 best_cand = sub_cand
            #         if tag1 == False:
            #             continue
            #         if s_name not in file_to_candidates:
            #             file_to_candidates[s_name] = [best_cand]
            #         elif s_name in file_to_candidates:
            #             file_to_candidates[s_name].append(best_cand)
            #     with open('./file_to_candidates.json', 'w') as fp:
            #         json.dump(file_to_candidates, fp)
            #     # print(file_to_candidates)


            # for fname, cand_list in tqdm.tqdm(file_to_candidates.items()):
            #     for i in range(len(cand_list)):
            #         if cand_list[i] == '':
            #             continue
            #         sub_query += 1.0
            #         if i == len(cand_list) - 1:
            #             surface_files.append(cand_list[i])
            #             break
            #         f_path = './mutated_candidates/mutated_code1/' + cand_list[i]
            #         f_code = open(f_path, 'r').read().strip()
            #         predict, _, _ = get_prediction(f_code, tokenizer, model, config)
            #         if predict != int(file_to_label[fname]):
            #             surface_files.append(cand_list[i])
            #             break

            # print(len(surface_files))
            '''通过ADHO和EDAO生成新种子'''
            mutatant_seed = {'1': '0', '1_12': '1', '12': '2'}
            for key, value in semantic_mutatants.items():
                mutatant_seed[value] = key
            
            all_mutated_files = {}
            for i in tqdm.tqdm(range(31)):
                if str(i) in surface_mutatants:
                    source_path = './mutated_candidates/true_mutated' + surface_mutatants[str(i)] + '.txt'
                    surface_mutated_files[str(i)] = get_file_list(source_path, [])
                    all_mutated_files[str(i)] = get_file_list(source_path, [])
                elif str(i) in syntax_mutatants:
                    m_seeds = syntax_mutatants[str(i)].split('_')
                    if len(m_seeds) == 1:
                        source_path = './mutated_candidates/true_mutated' + syntax_mutatants[str(i)] + '.txt'
                        syntax_mutated_files[str(i)] = get_file_list(source_path, [])
                        all_mutated_files[str(i)] = get_file_list(source_path, [])
                    else:
                        syntax_mutated_files[str(i)] = []
                        all_mutated_files[str(i)] = []
                        target_path = './mutated_candidates/true_mutated' + syntax_mutatants[str(i)] + '.txt'
                        target_mutatants = get_file_list(target_path, [])
                        source_path = './initial_code/'
                        tmp_path = './mutated_candidates/mutated_code' + syntax_mutatants[str(i)][:-(len(m_seeds[-1])+1)]
                        for m_file in target_mutatants:
                            if m_file in all_mutated_files[mutatant_seed[syntax_mutatants[str(i)][:-(len(m_seeds[-1])+1)]]]:
                                if fit_function(m_file, source_path, tmp_path, tokenizer, model, config, syntax_mutatants, syntax_indices, i):
                                    syntax_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)

                elif str(i) in semantic_mutatants:
                    m_seeds = semantic_mutatants[str(i)].split('_')
                    if len(m_seeds) == 1:
                        source_path = './mutated_candidates/true_mutated' + semantic_mutatants[str(i)] + '.txt'
                        semantic_mutated_files[str(i)] = get_file_list(source_path, [])
                        all_mutated_files[str(i)] = get_file_list(source_path, [])
                    elif len(m_seeds) == 2:
                        semantic_mutated_files[str(i)] = []
                        all_mutated_files[str(i)] = []
                        target_path = './mutated_candidates/true_mutated' + semantic_mutatants[str(i)] + '.txt'
                        target_mutatants = get_file_list(target_path, [])
                        source_path = './initial_code/'
                        tmp_path = './mutated_candidates/mutated_code' + semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]
                        for m_file in target_mutatants:
                            if m_file in all_mutated_files[mutatant_seed[semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]]]:
                                if i <= 9 and fit_function(m_file, source_path, tmp_path, tokenizer, model, config, semantic_mutatants, semantic_indices, i):
                                    semantic_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)
                                elif i > 9 and fit_function(m_file, source_path, tmp_path, tokenizer, model, config, semantic_mutatants, semantic_indices, i, True):
                                    semantic_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)
                    else:
                        semantic_mutated_files[str(i)] = []
                        all_mutated_files[str(i)] = []
                        target_path = './mutated_candidates/true_mutated' + semantic_mutatants[str(i)] + '.txt'
                        target_mutatants = get_file_list(target_path, [])
                        source_path = './mutated_candidates/mutated_code' + semantic_mutatants[str(i)][:-(len(m_seeds[-2])+1+len(m_seeds[-1])+1)] + '/'
                        # source_path = './initial_code/'
                        tmp_path = './mutated_candidates/mutated_code' + semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]
                        for m_file in target_mutatants:
                            if m_file in all_mutated_files[mutatant_seed[semantic_mutatants[str(i)][:-(len(m_seeds[-1])+1)]]]:
                                if fit_function(m_file, source_path, tmp_path, tokenizer, model, config, semantic_mutatants, semantic_indices, i, True):
                                    semantic_mutated_files[str(i)].append(m_file)
                                    all_mutated_files[str(i)].append(m_file)

            # for i in tqdm.tqdm(range(31)):
            #     if str(i) in surface_mutatants:
            #         source_path = './mutated_candidates/true_mutated' + surface_mutatants[str(i)] + '.txt'
            #         surface_mutated_files[str(i)] = get_file_list(source_path, [])
            #     elif str(i) in syntax_mutatants:
            #         source_path = './mutated_candidates/true_mutated' + syntax_mutatants[str(i)] + '.txt'
            #         syntax_mutated_files[str(i)] = get_file_list(source_path, [])
            #     elif str(i) in semantic_mutatants:
            #         source_path = './mutated_candidates/true_mutated' + semantic_mutatants[str(i)] + '.txt'
            #         semantic_mutated_files[str(i)] = get_file_list(source_path, [])
            # # print(true_mutated_files['14'])

            surface_seeds = {}
            syntax_seeds = {}
            semantic_seeds = {}
            for file in tqdm.tqdm(surface_files):
                t_file = file.split('-')[0] + '.c'
                for k, v in surface_mutated_files.items():
                    get_initial_seeds(file, t_file, k, v, surface_seeds)
                for k, v in syntax_mutated_files.items():
                    get_initial_seeds(file, t_file, k, v, syntax_seeds)
                for k, v in semantic_mutated_files.items():
                    get_initial_seeds(file, t_file, k, v, semantic_seeds)
            with open('./surface_seeds.json', 'w') as fp:
                json.dump(surface_seeds, fp)
            with open('./syntax_seeds.json', 'w') as fp:
                json.dump(syntax_seeds, fp)
            with open('./semantic_seeds.json', 'w') as fp:
                json.dump(semantic_seeds, fp)

        # print(initial_seeds)
        # for k, v in initial_seeds.items():
        #     if k == 'a553c6a347d3d28d7ee44c3df3d5c4ee780dba23_10398.c' or k.split('-')[0]+'.c' == 'a553c6a347d3d28d7ee44c3df3d5c4ee780dba23_10398.c':
        #         print(k)
    surface_candidates = {}
    syntax_candidates = {}
    semantic_candidates = {}
    # atten_candidates = {}
    get_candidates(surface_seeds, tokenizer, model, config, surface_candidates, surface_indices, surface_mutatants)   
    get_candidates(syntax_seeds, tokenizer, model, config, syntax_candidates, syntax_indices, syntax_mutatants)
    get_candidates(semantic_seeds, tokenizer, model, config, semantic_candidates, semantic_indices, semantic_mutatants)

    # with open('surface_candidates.json', 'w') as fp:
    #     json.dump(surface_candidates, fp)
    print(len(surface_candidates), len(syntax_candidates), len(semantic_candidates)) 
    return surface_candidates, syntax_candidates, semantic_candidates, sub_query
    

def testing(surface_candidates, syntax_candidates, semantic_candidates, file_to_label, sub_query):
    query = 0.0
    testcases_num = 0.0
    success = 0.0
    
    adversarial_example = open('adversarial_examples.txt', 'w')
    syntax_initial_files = []
    for fname, features in tqdm.tqdm(surface_candidates.items()):
         for i, sub_features in enumerate(features):
            predict = sub_features[-1]
            query += 1.0
            if sub_features[-1] != int(file_to_label[fname]):
                testcases_num += 1.0
                success += 1.0
                adversarial_example.write(sub_features[0]+'\n')
                # subprocess.check_output('cp '+sub_features[0]+' ../CodeBERT/test_set_adv', shell=True)
                break
            elif fname not in syntax_initial_files:
                syntax_initial_files.append(fname)

    semantic_initial_files = []
    for fname in tqdm.tqdm(syntax_initial_files):
        if fname in syntax_candidates:
            for i, sub_features in enumerate(syntax_candidates[fname]):
                predict = sub_features[-1]
                query += 1.0
                if sub_features[-1] != int(file_to_label[fname]):
                    testcases_num += 1.0
                    success += 1.0
                    adversarial_example.write(sub_features[0]+'\n')
                    # subprocess.check_output('cp '+sub_features[0]+' ../CodeBERT/test_set_adv', shell=True)
                    break
                elif fname not in semantic_initial_files:
                    semantic_initial_files.append(fname)
        else:
            semantic_initial_files.append(fname)
    
    print(len(semantic_initial_files))
    cnt = 0
    for fname in tqdm.tqdm(semantic_initial_files):
        if fname in semantic_candidates:
            for i, sub_features in enumerate(semantic_candidates[fname]):
                predict = sub_features[-1]
                query += 1.0
                if sub_features[-1] != int(file_to_label[fname]):
                    testcases_num += 1.0
                    success += 1.0
                    adversarial_example.write(sub_features[0]+'\n')
                    # subprocess.check_output('cp '+sub_features[0]+' ../CodeBERT/test_set_adv', shell=True)
                    break
                elif i == 0:
                    cnt += 1
                    testcases_num += 1.0
    adversarial_example.close()
    print(cnt)   
    print('[+] Results: query numbr:{}, success examples:{}, all testcases:{}, attacking successful rate:{}'.format(query+sub_query, success, testcases_num, success / testcases_num))
    # testing_result.close()
            


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='Encoder', help='model name')
    # parser.add_argument('--level', type=str, default='surface')
    # parser.add_argument('--d_type', type=str, default='a_distance')
    args = parser.parse_args()

    model_name = args.model
    X = import_module('module.' + model_name)
    config = X.Config()
    device = config.device
    dir_name = './save_dict/'
    config.save_path = dir_name + 'CodeBERT_12.ckpt'

    model = X.Encoder(config)
    model = model.to(device)
    model.load_state_dict(torch.load(config.save_path))

    file_to_label = {}
    with open('code_labels.txt', 'r') as fp:
        all_lines = fp.read().strip().split('\n')
        for line in tqdm.tqdm(all_lines):
            fname = line.split('  ')[0]
            label  = line.split('  ')[-1]
            file_to_label[fname] = label
    
    # fuzzer(config, args, model, file_to_label)
    surface_candidates, syntax_candidates, semantic_candidates, sub_query = fuzzer(config, args, model, file_to_label)
    testing(surface_candidates, syntax_candidates, semantic_candidates, file_to_label, sub_query)

    # atten_candidates, sub_query = fuzzer('e_distance', config, args, model, file_to_label)
    # testing(2, atten_candidates, file_to_label, sub_query)

if __name__ == "__main__":
    main()