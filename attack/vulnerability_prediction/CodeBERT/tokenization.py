import os
import sys
import torch
from collections import Counter
import tqdm
import pickle as pkl
import json
import re
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import RobertaTokenizer


def get_data_split(pkl_path):
    db = pd.read_pickle(pkl_path)
    train_data, val_test_data = train_test_split(db, test_size=0.2, random_state=9, stratify=db['label'])
    # print(val_test_data)
    store_path = './data'
    if not os.path.exists(store_path):
        os.mkdir(store_path)

    train_set_path = store_path + '/train_set.pkl'
    # train_set = pd.DataFrame()
    train_data.to_pickle(train_set_path)
    
    val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=9, stratify=val_test_data['label'])
    val_set_path = store_path + '/val_set.pkl'
    test_set_path = store_path + '/test_set.pkl'
    val_data.to_pickle(val_set_path)
    test_data.to_pickle(test_set_path)

def get_feature_token(data_path, tokenizer, store_path):
    vec_pkl = []
    with open(data_path) as f:
        for line in tqdm.tqdm(f):
            js=json.loads(line.strip())
            id_ = js['commit_id'] + '_' + str(js['idx']) + '.c'
            fcode = js['func']
            with open('../valid_code/'+id_, 'w') as fp:
                fp.write(fcode.strip())
            label = js['target']
            fcode = re.sub(r'(\s)+', ' ', fcode).strip()
            ftokens = ''.join(fcode.split(' '))
            ftokens = tokenizer.tokenize(ftokens)[:510]
            ftokens = [tokenizer.cls_token] + ftokens + [tokenizer.sep_token]
            fids = tokenizer.convert_tokens_to_ids(ftokens)
            padding_length = 512 - len(fids)
            fids += [tokenizer.pad_token_id] * padding_length
            vec_pkl.append([id_, fids, label])
    # data_path = data_path.replace('.pkl', '_token.pkl')
    pkl.dump(vec_pkl, open(store_path, 'wb'))

if __name__ == "__main__":
    tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
    print('[+] transform tokens to vectors...')
    train_data_path = './dataset/train.jsonl'
    val_data_path = './dataset/valid.jsonl'
    test_data_path = './dataset/test.jsonl'
    get_feature_token(train_data_path, tokenizer, './data/train_set_token.pkl')
    get_feature_token(val_data_path, tokenizer, './data/val_set_token.pkl')
    get_feature_token(test_data_path, tokenizer, './data/test_set_token.pkl')
    # get_feature_token('./all_set.pkl', tokenizer)